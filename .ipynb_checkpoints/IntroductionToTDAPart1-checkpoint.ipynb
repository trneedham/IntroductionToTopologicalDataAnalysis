{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Topological Data Analysis\n",
    "\n",
    "This notebook contains examples illustrating basic concepts from the field of Topological Data Analysis. Please see the accompanying lecture slides for more explanation.\n",
    "\n",
    "To run this notebook, you need to install standard scientific computing libraries as well as specialized TDA packages `ripser` (which depends on `cython`) and `persim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Clustering Algorithms\n",
    "\n",
    "We demonstrate the performance of several clustering algorithms on the following toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages for creating toy datasets and plotting\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Create two datasets using above functions\n",
    "X1, y1 = make_circles(n_samples=500, noise = 0.05, factor=0.3, random_state = 3)\n",
    "X2, y2 = make_blobs(n_samples=500, centers=2, center_box = (-10,10), random_state=2)\n",
    "\n",
    "# Combine the toy datasets. The first one is scaled and translated.\n",
    "X = np.concatenate((3*X1-np.array([[12,0]]),X2),axis=0)\n",
    "\n",
    "# Plot the result\n",
    "plt.scatter(X[:,0],X[:,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $k$-Means Clustering\n",
    "\n",
    "One of the most basic clustering algorithms is *$k$-Means Clustering*. The function is included in `scikit-learn`. To run $k$-Means, one must choose $k$. This is a nontrivial task in *unsupervised* learning, where ground truth labels are not known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=2).fit(X) \n",
    "# Choose 2 clusters to start\n",
    "# It's pretty clear that this is not the correct choice\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1],c = kmeans.labels_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=3).fit(X) \n",
    "# 3 clusters gives something reasonable\n",
    "# Still not correct\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1],c = kmeans.labels_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No matter how many clusters we choose, $k$-Means will not be able to separate the concentric circles---the means of the circles are the same, so there is no hope!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=4).fit(X) \n",
    "\n",
    "plt.scatter(X[:,0],X[:,1],c = kmeans.labels_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN\n",
    "\n",
    "Another popular clustering alborithm is DBSCAN. DBSCAN will choose the number of clusters based on the data. There are different parameters to choose here -- `eps` and `min_samples` -- which have a big effect on the performance of the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "dbscan = DBSCAN(eps = 2, min_samples=5).fit(X)\n",
    "plt.scatter( X[:,0], X[:,1], c=dbscan.labels_ );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we tune the parameters correctly, DBSCAN will be able to separate the concentric circles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "dbscan = DBSCAN(eps = 1, min_samples=5).fit(X)\n",
    "plt.scatter( X[:,0], X[:,1], c=dbscan.labels_ );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering\n",
    "\n",
    "These clustering algorithms (and most others) have a common type of output -- they partition the data into labeled subsets. This misses 'finer scale' clustering behavior. I.e., the concentric circles seem to form their own subcluster, as do the 'blobs'. \n",
    "\n",
    "There is another style of clustering algorithm called *hierarchical clustering* which produces a summary of clustering behavior at all scales. The output here can be viewed as a *dendrogram*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage  \n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "linked = linkage(X, 'single')\n",
    "dendrogram(linked, labels = None, show_leaf_counts=False)\n",
    "ax.set_xticklabels([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above shows that there are two clusters at the largest scale. Each of those consists of two clusters at a smaller scale. Below this there are lots of very indistinct clusters, which can be read as 'noise'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels for the points can be extracted from the dendrogram via the *agglomerative clustering* algorithm. Like $k$-Means, the function requires us to choose a number of clusters. The difference is that we can now make a very educated guess, using the dendrogram as our guide.\n",
    "\n",
    "We can see the two biggest clusters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "ac = AgglomerativeClustering(n_clusters=2, linkage = 'single')\n",
    "ac.fit(X)\n",
    "plt.scatter(X[:,0],X[:,1],c=ac.labels_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or the 4 biggest clusters ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ac = AgglomerativeClustering(n_clusters=4, linkage = 'single')\n",
    "ac.fit(X)\n",
    "plt.scatter(X[:,0],X[:,1],c=ac.labels_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or the 5 biggest clusters, which looks like the output of DBSCAN ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ac = AgglomerativeClustering(n_clusters=5, linkage = 'single')\n",
    "ac.fit(X)\n",
    "plt.scatter(X[:,0],X[:,1],c=ac.labels_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or however many clusters we want, which will pick out increasingly fine information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac = AgglomerativeClustering(n_clusters=100, linkage = 'single')\n",
    "ac.fit(X)\n",
    "plt.scatter(X[:,0],X[:,1],c=ac.labels_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Computing Persistence Diagrams\n",
    "\n",
    "The main topological feature that is used in TDA is called a *persistence diagram*. In this example we compute persistence diagrams for several point clouds. We will use specialized Python packages for TDA called `ripser` and `persim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from ripser import ripser\n",
    "# ripser is a package for computing barcodes from Vietoris-Rips complexes\n",
    "from persim import plot_diagrams \n",
    "# persim is a package for displaying and computing distances between persistence diagrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Point Cloud\n",
    "\n",
    "We begin by defining a toy dataset. Let's just take a bunch of random points in the plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = np.random.random((100,2)) # Define a random point cloud of 100 points in the plane\n",
    "fig = plt.figure(figsize=(5,5)) # Create a figure to display the random point cloud\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(data[:, 0], data[:, 1]) # Plot the data on the axes. +b plots as a scatter plot of blue + signs.\n",
    "ax.axis('equal'); # Set the aspect ratio to use equal axis scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute the persistent homology of the dataset. This is done by applying the `ripser` function. Ripser has lots of options, such as range of dimensions to compute, type of data etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ripserData = ripser(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the output of ripser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ripserData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The types of things that ripser computed are listed under various 'keys'. Let's take a look at those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ripserData.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computation that we really care about is under the 'dgms' key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagrams = ripser(data, maxdim=2)['dgms'] # Pick off the dgms part of the ripser output\n",
    "print(diagrams) # Look at the the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the output of diagrams is a pair of arrays. These are points in the persistence diagram for degree-0 and degree-1 persistent homology, respectively. We can now plot these diagrams. This can be done on separate axes, or on the same axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_diagrams(diagrams[0]) # Just degree-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_diagrams(diagrams[1]) # Just degree-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_diagrams(diagrams) # Both degree-0 and degree-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Circles and Blobs\n",
    "\n",
    "Applying persistent homology to the dataset from Example 1, we observe the multiscale clustering structure as wel as the fact that there are circles in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "diagrams = ripser(X)['dgms']\n",
    "plot_diagrams(diagrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spheres\n",
    "\n",
    "Let's examine the persistent homology of point clouds on spheres. First we define a function to randomly sample from Euclidean spheres. Then we sample 250 points from a sphere in 3-space and plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to sample randomly from a sphere\n",
    "def sample_spherical(npoints, ndim=3):\n",
    "    sample = np.random.randn(ndim, npoints)\n",
    "    sample /= np.linalg.norm(sample, axis=0)\n",
    "    sample = sample.T\n",
    "    return sample\n",
    "\n",
    "# Randomly sample from a sphere with added noise\n",
    "def noisy_sample_spherical(npoints, ndim, noise_level = 0.01):\n",
    "    sphere = sample_spherical(npoints, ndim)\n",
    "    noise = np.random.multivariate_normal(ndim*[0], noise_level*np.eye(ndim), npoints)\n",
    "    sample = sphere + noise\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first look at a sphere sitting in $\\mathbb{R}^2$, a.k.a. a circle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = noisy_sample_spherical(100,2)\n",
    "# Generate the noisy circle.\n",
    "\n",
    "# Plot the data as a scatter plot.\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.plot(data[:, 0], data[:, 1], 'ob', label='Source samples');\n",
    "ax1.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgms = ripser(data)['dgms']\n",
    "plot_diagrams(dgms, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playing with parameters, we can make the circle more densely sampled or much noisier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numsamp = 200\n",
    "noise = .1\n",
    "\n",
    "data = noisy_sample_spherical(numsamp,ndim=2,noise_level=noise)\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.plot(data[:, 0], data[:, 1], 'ob');\n",
    "ax1.axis('equal')\n",
    "dgms = ripser(data)['dgms']\n",
    "fig2 = plt.figure(figsize=(5,5))\n",
    "plot_diagrams(dgms, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the persistence diagrams for a sphere in $\\mathbb{R}^3$. We can specify that we want to compute homology up to degree-2. We then plot the persistence diagrams on the same axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sphere = noisy_sample_spherical(250,3)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.scatter(sphere[:,0],sphere[:,1],sphere[:,2], c='b', marker='o');\n",
    "ax.set_aspect('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start0 = time.time()\n",
    "diagrams = ripser(sphere,maxdim=2)['dgms']\n",
    "plot_diagrams(diagrams)\n",
    "end0 = time.time()\n",
    "\n",
    "print('Computation Time: ' + str(end0 - start0) + ' seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe in the persistence diagrams that there is one highly persistent 0-cycle and a single 2-cycle. These reflect the topology of the sphere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3: Filtering an Image\n",
    "\n",
    "This example is adapted from one appearing in the documentation for Ripser https://github.com/scikit-tda/ripser.py.\n",
    "\n",
    "The goal is to apply these techniques to study the following stock image of a dragonfly wing, available at <a href = \"https://all-free-download.com/free-photos/download/dragonfly-wings_207742.html\">this link</a>\n",
    "\n",
    "<img src = \"dragonfly_wings.jpg\">\n",
    "\n",
    "If we think of each pixel value as the output of a function, this image can be understood as the graph of a surface. If white pixels correspond to large function values, then we can imagine that the center of each 'cell' in the wing corresponds to a local max of the function. If we take the negative of our function, each cell corresponds to a local min.\n",
    "\n",
    "We suspect that the degree-0 persistent homology of a sublevel set filtration of this graph would pick out these local mins as points with high persistence. The goal of this example is to test this intuition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the Image\n",
    "\n",
    "We begin by converting the image to true greyscale, then smoothing it (creating a smoother surface graph)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "\n",
    "wing_original = plt.imread(\"dragonfly_wings.jpg\")\n",
    "wing_grey = np.asarray(PIL.Image.fromarray(wing_original).convert('L'))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title('Original')\n",
    "plt.imshow(wing_original)\n",
    "plt.axis('off')\n",
    "plt.subplot(122)\n",
    "plt.title('Greyscale')\n",
    "plt.imshow(wing_grey, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we smooth, we will add a small amount of noise to each pixel value. This is a *hack* to make the pixel values unique. It will help us plot later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy import ndimage\n",
    "\n",
    "# Smooth the image using a moving average and add some random noise\n",
    "smoothed = ndimage.uniform_filter(wing_grey.astype(np.float64), size=20)\n",
    "smoothed += 0.01 * np.random.randn(*smoothed.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(121)\n",
    "plt.title('Greyscale')\n",
    "im = plt.imshow(wing_grey, cmap='gray')\n",
    "plt.colorbar(im, fraction=0.03)\n",
    "\n",
    "plt.subplot(122)\n",
    "im = plt.imshow(smoothed, cmap='gray')\n",
    "plt.title('Smoothed')\n",
    "plt.colorbar(im, fraction=0.03)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the sublevel set filtration for the image surface. Superlevel set filtrations for image data are built into `ripser` as `lower_star_img`. Recall that we take the negative of our surface to get white pixels to be local minima.\n",
    "\n",
    "We display the persistence diagram with 'birth time' on the $x$-axis and 'lifetime' on the $y$-axis (lifetime = death time - birth time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from ripser import lower_star_img\n",
    "\n",
    "dgm = lower_star_img(-smoothed)\n",
    "plot_diagrams(dgm, lifetime=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like something interesting happens for lifetime around 5. Let's threshold there, then plot the pixels generating the homology classes on top of the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 5\n",
    "# Create list of indices up to number of points in persistence diagram\n",
    "idxs = np.arange(dgm.shape[0])\n",
    "# Find indices of points with long persistence\n",
    "idxs = idxs[np.abs(dgm[:, 1] - dgm[:, 0]) > thresh]\n",
    "\n",
    "# Plot original wing image\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.imshow(wing_original)\n",
    "\n",
    "# Plot points generating high-persistence points in the diagram\n",
    "X, Y = np.meshgrid(np.arange(smoothed.shape[1]), np.arange(smoothed.shape[0]))\n",
    "X = X.flatten()\n",
    "Y = Y.flatten()\n",
    "for idx in idxs:\n",
    "    bidx = np.argmin(np.abs(smoothed + dgm[idx, 0]))\n",
    "    plt.scatter(X[bidx], Y[bidx], 20, 'k')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does a pretty good (not perfect!) job. We could play around with the noise levels and thresholds to see if we can get a better result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
